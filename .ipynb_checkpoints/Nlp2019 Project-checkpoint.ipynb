{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, GRU\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential \n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from qrnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    return cleanhtml(text.replace('\\n', ''))\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleaner = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleaner, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "def remove_punc(test_string):\n",
    "    pattern = re.compile(r\"[^\\u0E00-\\u0E7Fa-zA-Z' ]|^'|'$''\")\n",
    "    char_to_remove = [e for e in re.findall(pattern, test_string) if e != '|']\n",
    "    list_with_char_removed = [char for char in test_string if not char in char_to_remove]\n",
    "    result_string = ''.join(list_with_char_removed)\n",
    "    return result_string\n",
    "\n",
    "def tokenize(text):\n",
    "    text = clean_text(text)\n",
    "    text = remove_punc(text)\n",
    "    return [e.strip() for e in text.split('|') if len(e) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = [ e for e in os.listdir('./BEST2010/training_set/') if e[0] != '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for s in sets:\n",
    "    training_data_path = os.path.join('./BEST2010/training_set/', s)\n",
    "    training_files = [os.path.join(training_data_path, e) for e in os.listdir(training_data_path) if e[0] != '.']\n",
    "    \n",
    "    for i in range(len(training_files)):\n",
    "        f = open(training_files[i], \"r\")\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        for l in lines:\n",
    "            train.append(tokenize(l))\n",
    "            \n",
    "train = [e for e in train if len(e) > 1]\n",
    "for i in range(len(train)):\n",
    "    train[i] = [w for w in train[i] if len(w) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "for l in train:\n",
    "    for w in l:\n",
    "        if w not in dictionary:\n",
    "            dictionary[w] = 1\n",
    "        else:\n",
    "            dictionary[w] += 1\n",
    "            \n",
    "threshold = 10\n",
    "counts_words = sorted([(v, k) for (k, v) in dictionary.items()], reverse=True)\n",
    "\n",
    "words2idx = {}; idx = 0\n",
    "for e in (counts_words):\n",
    "    word = e[1]\n",
    "    if dictionary[word] > threshold:\n",
    "        words2idx[word] = idx + 1\n",
    "        idx += 1\n",
    "words2idx['keras_padding'] = 0\n",
    "\n",
    "MAX_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = []\n",
    "for l in train:\n",
    "    train_tokens.append([words2idx[w] for w in l if w in words2idx])\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in train_tokens:\n",
    "    for i in range(1, len(line)):\n",
    "        n_gram_sequence = line[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "del train_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 64, input_length=input_len))\n",
    "    \n",
    "    model.add(QRNN(256, window_size=2, dropout=0.1, return_sequences=True,\n",
    "               kernel_regularizer=l2(1e-4), bias_regularizer=l2(1e-4), \n",
    "               kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10)))\n",
    "    model.add(QRNN(256, window_size=2, dropout=0.1, return_sequences=False,\n",
    "           kernel_regularizer=l2(1e-4), bias_regularizer=l2(1e-4), \n",
    "           kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10)))\n",
    "    \n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(MAX_LENGTH, len(words2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    }
   ],
   "source": [
    "chk = ModelCheckpoint('QRNN_best.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "cb = [chk]\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(0.001))\n",
    "model.load_weights('QRNN_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = dict((v,k) for k,v in words2idx.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_second_best_idx(probs):\n",
    "    return np.arange(0, probs.shape[0])[np.argsort(probs) == probs.shape[0]-2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text, next_words, max_sequence_len, model):\n",
    "    for j in range(next_words):\n",
    "        token_list = [words2idx[w] for w in text.split()]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        \n",
    "        predicted = np.squeeze(model.predict(token_list, verbose=0))\n",
    "        \n",
    "        output_word = idx2word[np.argmax(predicted)]\n",
    "        if output_word in text.split(' ')[-5:]:\n",
    "            output_word = idx2word[get_second_best_idx(predicted)]\n",
    "        \n",
    "        text += \" \" + output_word\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ธนาคาร เพื่อ ประชาชน ใน พื้นที่ จังหวัด ชาย แดน ภาค ใต้ และ รัฐบาล ที่ มี การ จัดตั้ง ศาลสิ่งแวดล้อม และ ให้ ประชาชน ใน พื้นที่ จังหวัด ชาย แดน ภาค ใต้ และ ประชาชน ใน พื้นที่ จังหวัด ชาย แดน ภาค ใต้ ให้ ประชาชน เพื่อ ปรารถนา ใน การ ป้องกัน ปัญหา เบื่อหน่าย และ ไม่ ได้ รับ การ ร้องเรียน จาก เจ้าหน้าที่ ตำรวจ ได้ สั่งการ ให้ อาณา บ้าน พัก ที่ เกิด เหตุ'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = 'ธนาคาร เพื่อ ประชาชน'\n",
    "generate_text(test_text, 60, MAX_LENGTH, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import savefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"QRNN_2layers.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
